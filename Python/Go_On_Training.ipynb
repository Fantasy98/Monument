{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import keras.backend as K \n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import zipfile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Model and Weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = r\"E:\\Monument\\1000Epochs\\Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of image is (360, 640, 3)\n",
      "The Height is 360\n",
      "The Width is 640\n",
      "The Channel is 3\n"
     ]
    }
   ],
   "source": [
    "path = r\"E:\\Monument\\Train\\real\"\n",
    "img_name = \"15.jpg\"\n",
    "path = os.path.join(path,img_name)\n",
    "\n",
    "img = keras.utils.load_img(path)\n",
    "img_array = keras.utils.img_to_array(img)\n",
    "\n",
    "print(f\"The shape of image is {img_array.shape}\")\n",
    "# Sort out the basic parameters\n",
    "HEIGHT = img_array.shape[0]\n",
    "WIDTH = img_array.shape[1]\n",
    "CHANNEL = img_array.shape[2]\n",
    "print(f\"The Height is {HEIGHT}\")\n",
    "print(f\"The Width is {WIDTH}\")\n",
    "print(f\"The Channel is {CHANNEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "def Generator_Encoder():\n",
    "    input_layer = layers.Input(name=\"input\",shape=(HEIGHT,WIDTH,CHANNEL))\n",
    "    x = layers.Conv2D(32,(5,5),strides=(1,1),padding =\"same\",name= \"conv_1\",kernel_regularizer=\"l2\")(input_layer)\n",
    "    x = layers.LeakyReLU(name=\"leaky_1\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "\n",
    "    x = layers.Conv2D(64,(3,3),strides=(2,2),padding =\"same\",name= \"conv_2\",kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.BatchNormalization(name=\"norm_1\")(x)\n",
    "    x = layers.LeakyReLU(name=\"leaky_2\")(x)\n",
    "\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "\n",
    "    x = layers.Conv2D(128,(3,3),strides=(2,2),padding =\"same\",name= \"conv_3\",kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.BatchNormalization(name=\"norm_2\")(x)\n",
    "    x = layers.LeakyReLU(name=\"leaky_3\")(x)\n",
    "    \n",
    "    # x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    x = layers.Conv2D(240,(3,3),strides=(2,2),padding =\"same\",name= \"conv_4\",kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.BatchNormalization(name=\"norm_3\")(x)\n",
    "    x = layers.LeakyReLU(name=\"leaky_4\")(x)\n",
    "\n",
    "    # x = layers.Conv2D(512,(3,3),strides=(2,2),padding =\"same\",name= \"conv_5\",kernel_regularizer=\"l2\")(x)\n",
    "    # x = layers.BatchNormalization(name=\"norm_4\")(x)\n",
    "    # x = layers.LeakyReLU(name=\"leaky_5\")(x)\n",
    "\n",
    "    # x = layers.Conv2D(1024,(3,3),strides=(2,2),padding =\"same\",name= \"conv_6\",kernel_regularizer=\"l2\")(x)\n",
    "    # x = layers.BatchNormalization(name=\"norm_5\")(x)\n",
    "    # x = layers.LeakyReLU(name=\"leaky_6\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D(name = \"g_encoder_output\")(x)\n",
    "\n",
    "    return keras.Model(inputs =input_layer,outputs =x)\n",
    "\n",
    "### 1.2.2 Generator \n",
    "def Generator():\n",
    "    g_e  = Generator_Encoder()\n",
    "    input_layer = layers.Input(name=\"input\",shape=(HEIGHT,WIDTH,CHANNEL))\n",
    "\n",
    "    x = g_e(input_layer)\n",
    "    height = np.int64(HEIGHT//30)\n",
    "    width = np.int64(WIDTH//32)\n",
    "    x = layers.Dense(height*width*240,name=\"dense\")(x)\n",
    "    x = layers.Reshape((height,width,240),name=\"de_reshape\")(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(240,(4,4),strides =(2,2),padding = \"same\",name=\"deconv_1\",kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.LeakyReLU(name=\"de_leaky_1\")(x)\n",
    "\n",
    "    # x = layers.Conv2DTranspose(256,(3,3),strides =(2,2),padding = \"same\",name=\"deconv_2\",kernel_regularizer=\"l2\")(x)\n",
    "    # x = layers.LeakyReLU(name=\"de_leaky_2\")(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(128,(2,2),strides =(2,2),padding = \"same\",name=\"deconv_3\",kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.LeakyReLU(name=\"de_leaky_3\")(x)\n",
    "    \n",
    "\n",
    "    x = layers.Conv2DTranspose(64,(2,2),strides =(2,2),padding = \"same\",name=\"deconv_4\",kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.LeakyReLU(name=\"de_leaky_4\")(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(32,(2,2),strides =(2,2),padding = \"same\",name=\"deconv_5\",kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.LeakyReLU(name=\"de_leaky_5\")(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(16,(2,2),strides =(2,2),padding = \"same\",name=\"deconv_6\",kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.LeakyReLU(name=\"de_leaky_6\")(x)\n",
    "    \n",
    "\n",
    "    x = layers.Conv2DTranspose(CHANNEL,(1,1),strides= (1,1),padding =\"same\",name=\"decoder_deconv_output\",kernel_regularizer = \"l2\",activation=\"tanh\")(x)\n",
    "\n",
    "    x = layers.Resizing(HEIGHT,WIDTH)(x)\n",
    "    return keras.Model(inputs =input_layer,outputs=x)\n",
    "    \n",
    "    \n",
    "### 1.2.3 Encoder \n",
    "def Encoder():\n",
    "    input_layer = layers.Input(name=\"encoder_input\",shape=(HEIGHT,WIDTH,CHANNEL))\n",
    "    x = layers.Conv2D(32,(5,5),strides=(1,1),padding =\"same\",name= \"encoder_conv_1\",kernel_regularizer=\"l2\")(input_layer)\n",
    "    x = layers.LeakyReLU(name=\"encoder_leaky_1\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "\n",
    "    x = layers.Conv2D(64,(3,3),strides=(2,2),padding =\"same\",name= \"encoder_conv_2\",kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.BatchNormalization(name=\"encoder_norm_1\")(x)\n",
    "    x = layers.LeakyReLU(name=\"encoder_leaky_2\")(x)\n",
    "\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "\n",
    "    x = layers.Conv2D(128,(3,3),strides=(2,2),padding =\"same\",name= \"encoder_conv_3\",kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.BatchNormalization(name=\"encoder_norm_2\")(x)\n",
    "    x = layers.LeakyReLU(name=\"encoder_leaky_3\")(x)\n",
    "    \n",
    "    # x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    x = layers.Conv2D(240,(3,3),strides=(2,2),padding =\"same\",name= \"encoder_conv_4\",kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.BatchNormalization(name=\"encoder_norm_3\")(x)\n",
    "    x = layers.LeakyReLU(name=\"encoder_leaky_4\")(x)\n",
    "\n",
    "    # x = layers.Conv2D(512,(3,3),strides=(2,2),padding =\"same\",name= \"conv_5\",kernel_regularizer=\"l2\")(x)\n",
    "    # x = layers.BatchNormalization(name=\"norm_4\")(x)\n",
    "    # x = layers.LeakyReLU(name=\"leaky_5\")(x)\n",
    "\n",
    "    # x = layers.Conv2D(1024,(3,3),strides=(2,2),padding =\"same\",name= \"conv_6\",kernel_regularizer=\"l2\")(x)\n",
    "    # x = layers.BatchNormalization(name=\"norm_5\")(x)\n",
    "    # x = layers.LeakyReLU(name=\"leaky_6\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D(name = \"encoder_output\")(x)\n",
    "\n",
    "    return keras.Model(inputs =input_layer,outputs =x)\n",
    "\n",
    "## 1.3 Discrimator \n",
    "### 1.3.1 Feature Extractor\n",
    "def Feature_Extractor():\n",
    "    input_layer = layers.Input(name=\"extractor_input\",shape=(HEIGHT,WIDTH,CHANNEL))\n",
    "    x = layers.Conv2D(32,(5,5),strides=(1,1),padding =\"same\",name= \"extractor_conv_1\",kernel_regularizer=\"l2\")(input_layer)\n",
    "    x = layers.LeakyReLU(name=\"extractor_leaky_1\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "\n",
    "    x = layers.Conv2D(64,(3,3),strides=(2,2),padding =\"same\",name= \"extractor_conv_2\",kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.BatchNormalization(name=\"extractor_norm_1\")(x)\n",
    "    x = layers.LeakyReLU(name=\"extractor_leaky_2\")(x)\n",
    "\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "\n",
    "    x = layers.Conv2D(128,(3,3),strides=(2,2),padding =\"same\",name= \"extractor_conv_3\",kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.BatchNormalization(name=\"extractor_norm_2\")(x)\n",
    "    x = layers.LeakyReLU(name=\"extractor_leaky_3\")(x)\n",
    "    \n",
    "    # x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    x = layers.Conv2D(240,(3,3),strides=(2,2),padding =\"same\",name= \"extractor_conv_4\",kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.BatchNormalization(name=\"extractor_norm_3\")(x)\n",
    "    x = layers.LeakyReLU(name=\"extractor_leaky_4\")(x)\n",
    "\n",
    "    # x = layers.Conv2D(512,(3,3),strides=(2,2),padding =\"same\",name= \"conv_5\",kernel_regularizer=\"l2\")(x)\n",
    "    # x = layers.BatchNormalization(name=\"norm_4\")(x)\n",
    "    # x = layers.LeakyReLU(name=\"leaky_5\")(x)\n",
    "\n",
    "    # x = layers.Conv2D(1024,(3,3),strides=(2,2),padding =\"same\",name= \"conv_6\",kernel_regularizer=\"l2\")(x)\n",
    "    # x = layers.BatchNormalization(name=\"norm_5\")(x)\n",
    "    # x = layers.LeakyReLU(name=\"leaky_6\")(x)\n",
    "\n",
    "    # x = layers.GlobalAveragePooling2D(name = \"extractor_output\")(x)\n",
    "\n",
    "    return keras.Model(inputs =input_layer,outputs =x)\n",
    "\n",
    "### 1.3.2 Discrimator \n",
    "def Discrimator():\n",
    "    feature_extractor = Feature_Extractor()\n",
    "\n",
    "    input_layer= layers.Input(name=\"input\",shape=(HEIGHT,WIDTH,CHANNEL))\n",
    "    x = feature_extractor(input_layer)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D(name=\"glb_avg\")(x)\n",
    "    x = layers.Dense(1,activation = \"sigmoid\",name=\"d_output\")(x)\n",
    "    return keras.Model(input_layer,x)\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "g_e = Generator_Encoder()\n",
    "##\n",
    "g = Generator()\n",
    "\n",
    "##\n",
    "encoder = Encoder()\n",
    "\n",
    "##\n",
    "\n",
    "feature_extractor = Feature_Extractor()\n",
    "\n",
    "##\n",
    "\n",
    "d = Discrimator()\n",
    "\n",
    "\n",
    "class AdvLoss(keras.layers.Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(AdvLoss,self).__init__(**kwargs)\n",
    "    def call(self,x,mask=None):\n",
    "        ori_feature = feature_extractor(x[0])\n",
    "        gan_feature = feature_extractor(x[1])\n",
    "\n",
    "        return K.mean(K.square(ori_feature-K.mean(gan_feature,axis=0) ) )\n",
    "\n",
    "    def get_output_shape_for(self,input_shape):\n",
    "        return (input_shape[0][0],3)\n",
    "\n",
    "\n",
    "class CntLoss(keras.layers.Layer):\n",
    "    def __init__(self,**kwargs) :\n",
    "        super(CntLoss,self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self,x,mask=None):\n",
    "        ori = x[0]\n",
    "        gan = x[1]\n",
    "\n",
    "        return K.mean(K.abs(ori-gan))\n",
    "\n",
    "    def get_output_shape_for(self,input_shape):\n",
    "        return (input_shape[0][0],3)\n",
    "\n",
    "class EncLoss(keras.layers.Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(EncLoss,self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self,x,mask=None):\n",
    "        ori = x[0]\n",
    "        gan = x[1]\n",
    "        \n",
    "        return K.mean(K.square( g_e(ori)-encoder(gan) ))\n",
    "    \n",
    "    def get_output_shape_for(self,input_shape):\n",
    "        return (input_shape[0][0],3)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "g = keras.models.load_model(r\"E:\\Monument\\1000Epochs\\Model\\content\\Model1\\GAN\")\n",
    "ge = keras.models.load_model(r\"E:\\Monument\\1000Epochs\\Model\\content\\Model1\\GAN_Encoder\")\n",
    "d = keras.models.load_model(r\"E:\\Monument\\1000Epochs\\Model\\content\\Model1\\Discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 360, 640, 3)]     0         \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 240)               373232    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 57600)             13881600  \n",
      "                                                                 \n",
      " de_reshape (Reshape)        (None, 12, 20, 240)       0         \n",
      "                                                                 \n",
      " deconv_1 (Conv2DTranspose)  (None, 24, 40, 240)       921840    \n",
      "                                                                 \n",
      " de_leaky_1 (LeakyReLU)      (None, 24, 40, 240)       0         \n",
      "                                                                 \n",
      " deconv_3 (Conv2DTranspose)  (None, 48, 80, 128)       123008    \n",
      "                                                                 \n",
      " de_leaky_3 (LeakyReLU)      (None, 48, 80, 128)       0         \n",
      "                                                                 \n",
      " deconv_4 (Conv2DTranspose)  (None, 96, 160, 64)       32832     \n",
      "                                                                 \n",
      " de_leaky_4 (LeakyReLU)      (None, 96, 160, 64)       0         \n",
      "                                                                 \n",
      " deconv_5 (Conv2DTranspose)  (None, 192, 320, 32)      8224      \n",
      "                                                                 \n",
      " de_leaky_5 (LeakyReLU)      (None, 192, 320, 32)      0         \n",
      "                                                                 \n",
      " deconv_6 (Conv2DTranspose)  (None, 384, 640, 16)      2064      \n",
      "                                                                 \n",
      " de_leaky_6 (LeakyReLU)      (None, 384, 640, 16)      0         \n",
      "                                                                 \n",
      " decoder_deconv_output (Conv  (None, 384, 640, 3)      51        \n",
      " 2DTranspose)                                                    \n",
      "                                                                 \n",
      " resizing (Resizing)         (None, 360, 640, 3)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,342,851\n",
      "Trainable params: 15,341,987\n",
      "Non-trainable params: 864\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Generate Path\n",
    "path = 'E:\\\\Monument\\\\Train\\\\real'\n",
    "file_list = os.listdir(path)\n",
    "path_list = [os.path.join(path,name) for name in file_list]\n",
    "\n",
    "\n",
    "## Shuffle the dataset \n",
    "import random\n",
    "random.shuffle(path_list)\n",
    "\n",
    "## generate data set \n",
    "from keras.utils import load_img,img_to_array\n",
    "Dataset = [img_to_array(load_img(image))/255 for image in path_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e450050b432e843bda3c41bf3272c133bfc370a7003f3e377e27f87a49ce1127"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
